{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dac0d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import itertools\n",
    "import argparse\n",
    "import collections\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from io import open\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n",
    "# from pytorch_pretrained_bert.modeling import BertForQuestionAnswering, BertConfig\n",
    "from modeling import BertForQuestionAnswering_Quant as BertForQuestionAnswering\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "from pytorch_pretrained_bert.tokenization import (\n",
    "    BasicTokenizer, BertTokenizer, whitespace_tokenize)\n",
    "\n",
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145e08fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FIT_utils import *\n",
    "from run_squad import read_squad_examples, convert_examples_to_features\n",
    "import matplotlib.pyplot as plt\n",
    "from quant_modules import QuantLinear, QuantLinear_Act, QuantEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc4e847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d52b8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = type('MyClass', (object,), {'content':{}})()\n",
    "args.train_file = '/home/ben/Documents/CERN/rebuttal_iclr/SQUAD/train-v1.1.json'\n",
    "args.bert_model = 'bert-base-uncased'\n",
    "args.do_lower_case = True\n",
    "args.train_batch_size = 12\n",
    "args.max_seq_length = 384\n",
    "args.doc_stride = 128\n",
    "args.max_query_length = 64\n",
    "args.version_2_with_negative=False\n",
    "args.config = None\n",
    "args.config_dir = None\n",
    "args.local_rank=-1\n",
    "args.bit_options = [4,8,32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b81d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "        args.bert_model, do_lower_case=args.do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a529c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = read_squad_examples(\n",
    "            input_file=args.train_file,\n",
    "            is_training=True,\n",
    "            version_2_with_negative=args.version_2_with_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5d69685",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-11-09 02:36:04,273 modeling.py:372] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/ben/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "[2022-11-09 02:36:04,274 modeling.py:380] extracting archive file /home/ben/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmps2916wsy\n",
      "[2022-11-09 02:36:07,691 modeling.py:452] Weights of BertForQuestionAnswering_Quant not initialized from pretrained model: ['bert.embeddings.word_embeddings.x_min', 'bert.embeddings.word_embeddings.x_max', 'bert.embeddings.position_embeddings.x_min', 'bert.embeddings.position_embeddings.x_max', 'bert.embeddings.token_type_embeddings.x_min', 'bert.embeddings.token_type_embeddings.x_max', 'bert.encoder.layer.0.attention.self.query.x_min', 'bert.encoder.layer.0.attention.self.query.x_max', 'bert.encoder.layer.0.attention.self.key.x_min', 'bert.encoder.layer.0.attention.self.key.x_max', 'bert.encoder.layer.0.attention.self.value.x_min', 'bert.encoder.layer.0.attention.self.value.x_max', 'bert.encoder.layer.0.attention.output.dense.x_min', 'bert.encoder.layer.0.attention.output.dense.x_max', 'bert.encoder.layer.0.intermediate.dense.x_min', 'bert.encoder.layer.0.intermediate.dense.x_max', 'bert.encoder.layer.0.output.dense.x_min', 'bert.encoder.layer.0.output.dense.x_max', 'bert.encoder.layer.1.attention.self.query.x_min', 'bert.encoder.layer.1.attention.self.query.x_max', 'bert.encoder.layer.1.attention.self.key.x_min', 'bert.encoder.layer.1.attention.self.key.x_max', 'bert.encoder.layer.1.attention.self.value.x_min', 'bert.encoder.layer.1.attention.self.value.x_max', 'bert.encoder.layer.1.attention.output.dense.x_min', 'bert.encoder.layer.1.attention.output.dense.x_max', 'bert.encoder.layer.1.intermediate.dense.x_min', 'bert.encoder.layer.1.intermediate.dense.x_max', 'bert.encoder.layer.1.output.dense.x_min', 'bert.encoder.layer.1.output.dense.x_max', 'bert.encoder.layer.2.attention.self.query.x_min', 'bert.encoder.layer.2.attention.self.query.x_max', 'bert.encoder.layer.2.attention.self.key.x_min', 'bert.encoder.layer.2.attention.self.key.x_max', 'bert.encoder.layer.2.attention.self.value.x_min', 'bert.encoder.layer.2.attention.self.value.x_max', 'bert.encoder.layer.2.attention.output.dense.x_min', 'bert.encoder.layer.2.attention.output.dense.x_max', 'bert.encoder.layer.2.intermediate.dense.x_min', 'bert.encoder.layer.2.intermediate.dense.x_max', 'bert.encoder.layer.2.output.dense.x_min', 'bert.encoder.layer.2.output.dense.x_max', 'bert.encoder.layer.3.attention.self.query.x_min', 'bert.encoder.layer.3.attention.self.query.x_max', 'bert.encoder.layer.3.attention.self.key.x_min', 'bert.encoder.layer.3.attention.self.key.x_max', 'bert.encoder.layer.3.attention.self.value.x_min', 'bert.encoder.layer.3.attention.self.value.x_max', 'bert.encoder.layer.3.attention.output.dense.x_min', 'bert.encoder.layer.3.attention.output.dense.x_max', 'bert.encoder.layer.3.intermediate.dense.x_min', 'bert.encoder.layer.3.intermediate.dense.x_max', 'bert.encoder.layer.3.output.dense.x_min', 'bert.encoder.layer.3.output.dense.x_max', 'bert.encoder.layer.4.attention.self.query.x_min', 'bert.encoder.layer.4.attention.self.query.x_max', 'bert.encoder.layer.4.attention.self.key.x_min', 'bert.encoder.layer.4.attention.self.key.x_max', 'bert.encoder.layer.4.attention.self.value.x_min', 'bert.encoder.layer.4.attention.self.value.x_max', 'bert.encoder.layer.4.attention.output.dense.x_min', 'bert.encoder.layer.4.attention.output.dense.x_max', 'bert.encoder.layer.4.intermediate.dense.x_min', 'bert.encoder.layer.4.intermediate.dense.x_max', 'bert.encoder.layer.4.output.dense.x_min', 'bert.encoder.layer.4.output.dense.x_max', 'bert.encoder.layer.5.attention.self.query.x_min', 'bert.encoder.layer.5.attention.self.query.x_max', 'bert.encoder.layer.5.attention.self.key.x_min', 'bert.encoder.layer.5.attention.self.key.x_max', 'bert.encoder.layer.5.attention.self.value.x_min', 'bert.encoder.layer.5.attention.self.value.x_max', 'bert.encoder.layer.5.attention.output.dense.x_min', 'bert.encoder.layer.5.attention.output.dense.x_max', 'bert.encoder.layer.5.intermediate.dense.x_min', 'bert.encoder.layer.5.intermediate.dense.x_max', 'bert.encoder.layer.5.output.dense.x_min', 'bert.encoder.layer.5.output.dense.x_max', 'bert.encoder.layer.6.attention.self.query.x_min', 'bert.encoder.layer.6.attention.self.query.x_max', 'bert.encoder.layer.6.attention.self.key.x_min', 'bert.encoder.layer.6.attention.self.key.x_max', 'bert.encoder.layer.6.attention.self.value.x_min', 'bert.encoder.layer.6.attention.self.value.x_max', 'bert.encoder.layer.6.attention.output.dense.x_min', 'bert.encoder.layer.6.attention.output.dense.x_max', 'bert.encoder.layer.6.intermediate.dense.x_min', 'bert.encoder.layer.6.intermediate.dense.x_max', 'bert.encoder.layer.6.output.dense.x_min', 'bert.encoder.layer.6.output.dense.x_max', 'bert.encoder.layer.7.attention.self.query.x_min', 'bert.encoder.layer.7.attention.self.query.x_max', 'bert.encoder.layer.7.attention.self.key.x_min', 'bert.encoder.layer.7.attention.self.key.x_max', 'bert.encoder.layer.7.attention.self.value.x_min', 'bert.encoder.layer.7.attention.self.value.x_max', 'bert.encoder.layer.7.attention.output.dense.x_min', 'bert.encoder.layer.7.attention.output.dense.x_max', 'bert.encoder.layer.7.intermediate.dense.x_min', 'bert.encoder.layer.7.intermediate.dense.x_max', 'bert.encoder.layer.7.output.dense.x_min', 'bert.encoder.layer.7.output.dense.x_max', 'bert.encoder.layer.8.attention.self.query.x_min', 'bert.encoder.layer.8.attention.self.query.x_max', 'bert.encoder.layer.8.attention.self.key.x_min', 'bert.encoder.layer.8.attention.self.key.x_max', 'bert.encoder.layer.8.attention.self.value.x_min', 'bert.encoder.layer.8.attention.self.value.x_max', 'bert.encoder.layer.8.attention.output.dense.x_min', 'bert.encoder.layer.8.attention.output.dense.x_max', 'bert.encoder.layer.8.intermediate.dense.x_min', 'bert.encoder.layer.8.intermediate.dense.x_max', 'bert.encoder.layer.8.output.dense.x_min', 'bert.encoder.layer.8.output.dense.x_max', 'bert.encoder.layer.9.attention.self.query.x_min', 'bert.encoder.layer.9.attention.self.query.x_max', 'bert.encoder.layer.9.attention.self.key.x_min', 'bert.encoder.layer.9.attention.self.key.x_max', 'bert.encoder.layer.9.attention.self.value.x_min', 'bert.encoder.layer.9.attention.self.value.x_max', 'bert.encoder.layer.9.attention.output.dense.x_min', 'bert.encoder.layer.9.attention.output.dense.x_max', 'bert.encoder.layer.9.intermediate.dense.x_min', 'bert.encoder.layer.9.intermediate.dense.x_max', 'bert.encoder.layer.9.output.dense.x_min', 'bert.encoder.layer.9.output.dense.x_max', 'bert.encoder.layer.10.attention.self.query.x_min', 'bert.encoder.layer.10.attention.self.query.x_max', 'bert.encoder.layer.10.attention.self.key.x_min', 'bert.encoder.layer.10.attention.self.key.x_max', 'bert.encoder.layer.10.attention.self.value.x_min', 'bert.encoder.layer.10.attention.self.value.x_max', 'bert.encoder.layer.10.attention.output.dense.x_min', 'bert.encoder.layer.10.attention.output.dense.x_max', 'bert.encoder.layer.10.intermediate.dense.x_min', 'bert.encoder.layer.10.intermediate.dense.x_max', 'bert.encoder.layer.10.output.dense.x_min', 'bert.encoder.layer.10.output.dense.x_max', 'bert.encoder.layer.11.attention.self.query.x_min', 'bert.encoder.layer.11.attention.self.query.x_max', 'bert.encoder.layer.11.attention.self.key.x_min', 'bert.encoder.layer.11.attention.self.key.x_max', 'bert.encoder.layer.11.attention.self.value.x_min', 'bert.encoder.layer.11.attention.self.value.x_max', 'bert.encoder.layer.11.attention.output.dense.x_min', 'bert.encoder.layer.11.attention.output.dense.x_max', 'bert.encoder.layer.11.intermediate.dense.x_min', 'bert.encoder.layer.11.intermediate.dense.x_max', 'bert.encoder.layer.11.output.dense.x_min', 'bert.encoder.layer.11.output.dense.x_max', 'qa_outputs.weight', 'qa_outputs.bias']\n",
      "[2022-11-09 02:36:07,691 modeling.py:456] Weights from pretrained model not used in BertForQuestionAnswering_Quant: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "cache_dir = os.path.join(\n",
    "    str(PYTORCH_PRETRAINED_BERT_CACHE),\n",
    "    'distributed_{}'.format(args.local_rank))\n",
    "model = BertForQuestionAnswering.from_pretrained(\n",
    "    args.bert_model,\n",
    "    cache_dir=cache_dir,\n",
    "    config_dir=args.config_dir,\n",
    "    config=args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9775edd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering_Quant(\n",
       "  (bert): BertModel_Quant(\n",
       "    (embeddings): BertEmbeddings_Quant(\n",
       "      (word_embeddings): QuantEmbedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): QuantEmbedding(512, 768)\n",
       "      (token_type_embeddings): QuantEmbedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder_Quant(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer_Quant(\n",
       "          (attention): BertAttention_Quant(\n",
       "            (self): BertSelfAttention_Quant(\n",
       "              (query): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput_Quant(\n",
       "              (dense): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate_Quant(\n",
       "            (dense): QuantLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput_Quant(\n",
       "            (dense): QuantLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer_Quant(\n",
       "          (attention): BertAttention_Quant(\n",
       "            (self): BertSelfAttention_Quant(\n",
       "              (query): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput_Quant(\n",
       "              (dense): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate_Quant(\n",
       "            (dense): QuantLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput_Quant(\n",
       "            (dense): QuantLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer_Quant(\n",
       "          (attention): BertAttention_Quant(\n",
       "            (self): BertSelfAttention_Quant(\n",
       "              (query): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput_Quant(\n",
       "              (dense): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate_Quant(\n",
       "            (dense): QuantLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput_Quant(\n",
       "            (dense): QuantLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer_Quant(\n",
       "          (attention): BertAttention_Quant(\n",
       "            (self): BertSelfAttention_Quant(\n",
       "              (query): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput_Quant(\n",
       "              (dense): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate_Quant(\n",
       "            (dense): QuantLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput_Quant(\n",
       "            (dense): QuantLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer_Quant(\n",
       "          (attention): BertAttention_Quant(\n",
       "            (self): BertSelfAttention_Quant(\n",
       "              (query): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput_Quant(\n",
       "              (dense): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate_Quant(\n",
       "            (dense): QuantLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput_Quant(\n",
       "            (dense): QuantLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer_Quant(\n",
       "          (attention): BertAttention_Quant(\n",
       "            (self): BertSelfAttention_Quant(\n",
       "              (query): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput_Quant(\n",
       "              (dense): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate_Quant(\n",
       "            (dense): QuantLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput_Quant(\n",
       "            (dense): QuantLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer_Quant(\n",
       "          (attention): BertAttention_Quant(\n",
       "            (self): BertSelfAttention_Quant(\n",
       "              (query): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput_Quant(\n",
       "              (dense): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate_Quant(\n",
       "            (dense): QuantLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput_Quant(\n",
       "            (dense): QuantLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer_Quant(\n",
       "          (attention): BertAttention_Quant(\n",
       "            (self): BertSelfAttention_Quant(\n",
       "              (query): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput_Quant(\n",
       "              (dense): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate_Quant(\n",
       "            (dense): QuantLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput_Quant(\n",
       "            (dense): QuantLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer_Quant(\n",
       "          (attention): BertAttention_Quant(\n",
       "            (self): BertSelfAttention_Quant(\n",
       "              (query): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput_Quant(\n",
       "              (dense): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate_Quant(\n",
       "            (dense): QuantLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput_Quant(\n",
       "            (dense): QuantLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer_Quant(\n",
       "          (attention): BertAttention_Quant(\n",
       "            (self): BertSelfAttention_Quant(\n",
       "              (query): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput_Quant(\n",
       "              (dense): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate_Quant(\n",
       "            (dense): QuantLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput_Quant(\n",
       "            (dense): QuantLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer_Quant(\n",
       "          (attention): BertAttention_Quant(\n",
       "            (self): BertSelfAttention_Quant(\n",
       "              (query): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput_Quant(\n",
       "              (dense): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate_Quant(\n",
       "            (dense): QuantLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput_Quant(\n",
       "            (dense): QuantLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer_Quant(\n",
       "          (attention): BertAttention_Quant(\n",
       "            (self): BertSelfAttention_Quant(\n",
       "              (query): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput_Quant(\n",
       "              (dense): QuantLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate_Quant(\n",
       "            (dense): QuantLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput_Quant(\n",
       "            (dense): QuantLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "563beb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train_features_file = args.train_file + '_{0}_{1}_{2}_{3}'.format(\n",
    "    list(filter(None, args.bert_model.split('/'))).pop(),\n",
    "    str(args.max_seq_length), str(args.doc_stride),\n",
    "    str(args.max_query_length))\n",
    "train_features = None\n",
    "try:\n",
    "    with open(cached_train_features_file, \"rb\") as reader:\n",
    "        train_features = pickle.load(reader)\n",
    "except BaseException:\n",
    "    train_features = convert_examples_to_features(\n",
    "        examples=train_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=args.max_seq_length,\n",
    "        doc_stride=args.doc_stride,\n",
    "        max_query_length=args.max_query_length,\n",
    "        is_training=True)\n",
    "    if args.local_rank == -1 or torch.distributed.get_rank() == 0:\n",
    "        with open(cached_train_features_file, \"wb\") as writer:\n",
    "            pickle.dump(train_features, writer)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features],\n",
    "                             dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features],\n",
    "                              dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features],\n",
    "                               dtype=torch.long)\n",
    "all_start_positions = torch.tensor(\n",
    "    [f.start_position for f in train_features], dtype=torch.long)\n",
    "all_end_positions = torch.tensor(\n",
    "    [f.end_position for f in train_features], dtype=torch.long)\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask,\n",
    "                           all_segment_ids, all_start_positions,\n",
    "                           all_end_positions)\n",
    "if args.local_rank == -1:\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "else:\n",
    "    train_sampler = DistributedSampler(train_data)\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=args.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c184fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "84934656\n",
      "0 bert.encoder.layer.0.attention.self.query 589824\n",
      "1 bert.encoder.layer.0.attention.self.key 589824\n",
      "2 bert.encoder.layer.0.attention.self.value 589824\n",
      "3 bert.encoder.layer.0.attention.output.dense 589824\n",
      "4 bert.encoder.layer.0.intermediate.dense 2359296\n",
      "5 bert.encoder.layer.0.output.dense 2359296\n",
      "6 bert.encoder.layer.1.attention.self.query 589824\n",
      "7 bert.encoder.layer.1.attention.self.key 589824\n",
      "8 bert.encoder.layer.1.attention.self.value 589824\n",
      "9 bert.encoder.layer.1.attention.output.dense 589824\n",
      "10 bert.encoder.layer.1.intermediate.dense 2359296\n",
      "11 bert.encoder.layer.1.output.dense 2359296\n",
      "12 bert.encoder.layer.2.attention.self.query 589824\n",
      "13 bert.encoder.layer.2.attention.self.key 589824\n",
      "14 bert.encoder.layer.2.attention.self.value 589824\n",
      "15 bert.encoder.layer.2.attention.output.dense 589824\n",
      "16 bert.encoder.layer.2.intermediate.dense 2359296\n",
      "17 bert.encoder.layer.2.output.dense 2359296\n",
      "18 bert.encoder.layer.3.attention.self.query 589824\n",
      "19 bert.encoder.layer.3.attention.self.key 589824\n",
      "20 bert.encoder.layer.3.attention.self.value 589824\n",
      "21 bert.encoder.layer.3.attention.output.dense 589824\n",
      "22 bert.encoder.layer.3.intermediate.dense 2359296\n",
      "23 bert.encoder.layer.3.output.dense 2359296\n",
      "24 bert.encoder.layer.4.attention.self.query 589824\n",
      "25 bert.encoder.layer.4.attention.self.key 589824\n",
      "26 bert.encoder.layer.4.attention.self.value 589824\n",
      "27 bert.encoder.layer.4.attention.output.dense 589824\n",
      "28 bert.encoder.layer.4.intermediate.dense 2359296\n",
      "29 bert.encoder.layer.4.output.dense 2359296\n",
      "30 bert.encoder.layer.5.attention.self.query 589824\n",
      "31 bert.encoder.layer.5.attention.self.key 589824\n",
      "32 bert.encoder.layer.5.attention.self.value 589824\n",
      "33 bert.encoder.layer.5.attention.output.dense 589824\n",
      "34 bert.encoder.layer.5.intermediate.dense 2359296\n",
      "35 bert.encoder.layer.5.output.dense 2359296\n",
      "36 bert.encoder.layer.6.attention.self.query 589824\n",
      "37 bert.encoder.layer.6.attention.self.key 589824\n",
      "38 bert.encoder.layer.6.attention.self.value 589824\n",
      "39 bert.encoder.layer.6.attention.output.dense 589824\n",
      "40 bert.encoder.layer.6.intermediate.dense 2359296\n",
      "41 bert.encoder.layer.6.output.dense 2359296\n",
      "42 bert.encoder.layer.7.attention.self.query 589824\n",
      "43 bert.encoder.layer.7.attention.self.key 589824\n",
      "44 bert.encoder.layer.7.attention.self.value 589824\n",
      "45 bert.encoder.layer.7.attention.output.dense 589824\n",
      "46 bert.encoder.layer.7.intermediate.dense 2359296\n",
      "47 bert.encoder.layer.7.output.dense 2359296\n",
      "48 bert.encoder.layer.8.attention.self.query 589824\n",
      "49 bert.encoder.layer.8.attention.self.key 589824\n",
      "50 bert.encoder.layer.8.attention.self.value 589824\n",
      "51 bert.encoder.layer.8.attention.output.dense 589824\n",
      "52 bert.encoder.layer.8.intermediate.dense 2359296\n",
      "53 bert.encoder.layer.8.output.dense 2359296\n",
      "54 bert.encoder.layer.9.attention.self.query 589824\n",
      "55 bert.encoder.layer.9.attention.self.key 589824\n",
      "56 bert.encoder.layer.9.attention.self.value 589824\n",
      "57 bert.encoder.layer.9.attention.output.dense 589824\n",
      "58 bert.encoder.layer.9.intermediate.dense 2359296\n",
      "59 bert.encoder.layer.9.output.dense 2359296\n",
      "60 bert.encoder.layer.10.attention.self.query 589824\n",
      "61 bert.encoder.layer.10.attention.self.key 589824\n",
      "62 bert.encoder.layer.10.attention.self.value 589824\n",
      "63 bert.encoder.layer.10.attention.output.dense 589824\n",
      "64 bert.encoder.layer.10.intermediate.dense 2359296\n",
      "65 bert.encoder.layer.10.output.dense 2359296\n",
      "66 bert.encoder.layer.11.attention.self.query 589824\n",
      "67 bert.encoder.layer.11.attention.self.key 589824\n",
      "68 bert.encoder.layer.11.attention.self.value 589824\n",
      "69 bert.encoder.layer.11.attention.output.dense 589824\n",
      "70 bert.encoder.layer.11.intermediate.dense 2359296\n",
      "71 bert.encoder.layer.11.output.dense 2359296\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fit_computerw \u001b[38;5;241m=\u001b[39m \u001b[43mFIT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpooler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mqa\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/CERN/rebuttal_iclr/QBERT/FIT_utils.py:16\u001b[0m, in \u001b[0;36mFIT.__init__\u001b[0;34m(self, model, device, data_loader, layer_filter)\u001b[0m\n\u001b[1;32m     14\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(data_loader))\n\u001b[1;32m     15\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(t\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch)\n\u001b[0;32m---> 16\u001b[0m input_ids, input_mask, segment_ids, label_ids \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# define a new function to compute loss values for both output_modes\u001b[39;00m\n\u001b[1;32m     19\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_ids, segment_ids, input_mask, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "fit_computerw = FIT(model, device, train_dataloader, ['pooler', 'qa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9569cfa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "EFw, EFa, fap, faa, param_ranges, act_ranges = fit_computerw.EF(model, train_dataloader, \n",
    "                                                               None, \n",
    "                                                               tol=1e-2, \n",
    "                                                               min_iterations=200,\n",
    "                                                               max_iterations=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eeefc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model\n",
    "cache_dir = os.path.join(\n",
    "    str(PYTORCH_PRETRAINED_BERT_CACHE),\n",
    "    'distributed_{}'.format(args.local_rank))\n",
    "model = BertForQuestionAnswering.from_pretrained(\n",
    "    args.bert_model,\n",
    "    cache_dir=cache_dir,\n",
    "    config_dir=args.config_dir,\n",
    "    config=args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1da5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b283b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_computera = FIT(model, device, train_dataloader, ['output', 'pooler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682bd27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, EFa, _, faa, _, act_ranges = fit_computera.EF(model, train_dataloader, \n",
    "                                                               None, \n",
    "                                                               tol=1e-2, \n",
    "                                                               min_iterations=200,\n",
    "                                                               max_iterations=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('W Trace')\n",
    "plt.plot(EFw/fit_computerw.param_nums,'o-', label='EF')\n",
    "plt.grid(True, which='both')\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb27823",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('A Trace')\n",
    "plt.plot(EFa/fit_computera.act_nums,'o-', label='EF')\n",
    "plt.grid(True, which='both')\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a2b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fap)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ac1f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(faa)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_computerw.Ra = act_ranges\n",
    "fit_computerw.EFa = EFa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(EFa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d79d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define useful layer hooks:\n",
    "def linear_flops_counter_hook(module, input, output):\n",
    "    input = input[0]\n",
    "    # pytorch checks dimensions, so here we don't care much\n",
    "    output_last_dim = output.shape[-1]\n",
    "    bias_flops = output_last_dim if module.bias is not None else 0\n",
    "    module.__flops__ += int(np.prod(input.shape) * output_last_dim + bias_flops)\n",
    "    \n",
    "def multihead_attention_counter_hook(multihead_attention_module, input, output):\n",
    "    flops = 0\n",
    "\n",
    "    q, k, v = input\n",
    "\n",
    "    batch_first = multihead_attention_module.batch_first \\\n",
    "        if hasattr(multihead_attention_module, 'batch_first') else False\n",
    "    if batch_first:\n",
    "        batch_size = q.shape[0]\n",
    "        len_idx = 1\n",
    "    else:\n",
    "        batch_size = q.shape[1]\n",
    "        len_idx = 0\n",
    "\n",
    "    dim_idx = 2\n",
    "\n",
    "    qdim = q.shape[dim_idx]\n",
    "    kdim = k.shape[dim_idx]\n",
    "    vdim = v.shape[dim_idx]\n",
    "\n",
    "    qlen = q.shape[len_idx]\n",
    "    klen = k.shape[len_idx]\n",
    "    vlen = v.shape[len_idx]\n",
    "\n",
    "    num_heads = multihead_attention_module.num_heads\n",
    "    assert qdim == multihead_attention_module.embed_dim\n",
    "\n",
    "    if multihead_attention_module.kdim is None:\n",
    "        assert kdim == qdim\n",
    "    if multihead_attention_module.vdim is None:\n",
    "        assert vdim == qdim\n",
    "\n",
    "    flops = 0\n",
    "\n",
    "    # Q scaling\n",
    "    flops += qlen * qdim\n",
    "\n",
    "    # Initial projections\n",
    "    flops += (\n",
    "        (qlen * qdim * qdim)  # QW\n",
    "        + (klen * kdim * kdim)  # KW\n",
    "        + (vlen * vdim * vdim)  # VW\n",
    "    )\n",
    "\n",
    "    if multihead_attention_module.in_proj_bias is not None:\n",
    "        flops += (qlen + klen + vlen) * qdim\n",
    "\n",
    "    # attention heads: scale, matmul, softmax, matmul\n",
    "    qk_head_dim = qdim // num_heads\n",
    "    v_head_dim = vdim // num_heads\n",
    "\n",
    "    head_flops = (\n",
    "        (qlen * klen * qk_head_dim)  # QK^T\n",
    "        + (qlen * klen)  # softmax\n",
    "        + (qlen * klen * v_head_dim)  # AV\n",
    "    )\n",
    "\n",
    "    flops += num_heads * head_flops\n",
    "\n",
    "    # final projection, bias is always enabled\n",
    "    flops += qlen * vdim * (vdim + 1)\n",
    "\n",
    "    flops *= batch_size\n",
    "    multihead_attention_module.__flops__ += int(flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b12b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODULES_MAPPING = {\n",
    "    nn.Linear: linear_flops_counter_hook,\n",
    "    QuantLinear: linear_flops_counter_hook,\n",
    "    nn.MultiheadAttention: multihead_attention_counter_hook\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaeae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_input_constructor(input_shape, tokenizer):\n",
    "    inp_seq = \"\"\n",
    "    for _ in range(input_shape[1] - 2):  # there are two special tokens [CLS] and [SEP]\n",
    "        inp_seq += tokenizer.pad_token  # let's use pad token to form a fake\n",
    "    # sequence for subsequent flops calculation\n",
    "\n",
    "    inputs = tokenizer([inp_seq] * input_shape[0], padding=True, truncation=True,\n",
    "                       return_tensors=\"pt\")\n",
    "    labels = torch.tensor([1] * input_shape[0])\n",
    "    # Batch size input_shape[0], sequence length input_shape[128]\n",
    "    inputs = dict(inputs)\n",
    "    inputs.update({\"labels\": labels})\n",
    "    return inputs\n",
    "\n",
    "def remove(layers):\n",
    "    for l in layers:\n",
    "        l.__flops_handle__.remove()\n",
    "        del l.__flops_handle__\n",
    "        del l.__flops__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "names = []\n",
    "for name, module in model.named_modules():\n",
    "    if type(module) in MODULES_MAPPING and 'pooler' not in name:\n",
    "        names.append(name)\n",
    "        layers.append(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5299cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in layers:\n",
    "    l.__flops__ = 0\n",
    "    l.__flops_handle__ = l.register_forward_hook(MODULES_MAPPING[type(l)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_dataloader))\n",
    "batch = tuple(t.to(device) for t in data)\n",
    "batch_size= len(batch)\n",
    "input_ids, input_mask, segment_ids, start_positions, end_positions = batch\n",
    "_ = model(input_ids, segment_ids, input_mask, start_positions, end_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09cb6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_fp32_bops = [l.__flops__ for l in layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b27e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l, b in zip(names, bert_fp32_bops):\n",
    "    print(l, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bert_fp32_bops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bops = np.sum(bert_fp32_bops)\n",
    "print(total_bops)\n",
    "print(total_bops * (8/32)**2)\n",
    "print(total_bops * (4/32)**2)\n",
    "print(total_bops * (8/32)*(4/32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f31f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up a bit of space to do the analysis\n",
    "del model\n",
    "del train_dataloader\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1030f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sensitivity = np.concatenate((EFw/fit_computerw.param_nums, EFa/fit_computera.act_nums), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c977071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [i for i in itertools.product(np.arange(len(total_sensitivity)+1), repeat=len(args.bit_options)) if sum(i)==len(total_sensitivity)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b1353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(configs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(total_sensitivity)\n",
    "bops_acc = []\n",
    "fit_values_acc = []\n",
    "w_configs_acc = []\n",
    "a_configs_acc = []\n",
    "min_FIT = np.inf\n",
    "# min_criterion = 10.5e10\n",
    "best = None\n",
    "# for _, c in enumerate(configs):\n",
    "#     # Expand c to get bit allocations matching the amount of layers to quantize\n",
    "#     bit_allocations = [np.repeat(args.bit_options, c)[np.where(order==i)[0][0]] for i in range(len(total_sensitivity))]\n",
    "    \n",
    "    \n",
    "#     w_config = bit_allocations[0:72]\n",
    "#     a_config = bit_allocations[72:]\n",
    "    \n",
    "#     fit_value = fit_computerw.FIT(np.array(w_config), np.array(a_config))\n",
    "    \n",
    "#     bops = 0\n",
    "#     a_config.insert(0, 32)\n",
    "    \n",
    "#     a_indx = 0\n",
    "#     for i, (bo, name, wb) in enumerate(zip(bert_fp32_bops[:-1], fit_computerw.names, w_config)):\n",
    "#         if name in fit_computera.names:\n",
    "#             bops += bo*a_config[a_indx]*wb*(1/32)**2\n",
    "#             a_indx += 1\n",
    "#         else:\n",
    "#             bops += bo*wb*8*(1/32)**2\n",
    "    \n",
    "    \n",
    "# #     for i, (wb, ab) in enumerate(zip(w_config, a_config)):\n",
    "# #         bops += bert_fp32_bops[i]*wb*ab*(1/32)**2\n",
    "        \n",
    "# #     possible_configs.append((bops, fit_value, w_config, a_config[1:]))\n",
    "#     bops_acc.append(bops)\n",
    "#     fit_values_acc.append(fit_value)\n",
    "#     w_configs_acc.append(w_config)\n",
    "#     a_configs_acc.append(a_config)\n",
    "    \n",
    "#     if bops < min_criterion and fit_value < min_FIT:\n",
    "#         min_FIT = fit_value\n",
    "#         best = (bops, fit_value, w_config, a_config[1:])\n",
    "        \n",
    "# print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3348bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate additional random configurations\n",
    "# min_FIT = np.inf\n",
    "min_criterion = 13030593057.0\n",
    "for i in range(2000):\n",
    "    w_config = list(np.random.choice(args.bit_options[:-1], 72, p=[0.9,0.1]))\n",
    "    a_config = list(np.random.choice(args.bit_options[:-1], 48, p=[0.7,0.3]))\n",
    "    \n",
    "    fit_value = fit_computerw.FIT(np.array(w_config), np.array(a_config))\n",
    "    \n",
    "    bops = 0\n",
    "    \n",
    "    a_indx = 0\n",
    "    for i, (bo, name, wb) in enumerate(zip(bert_fp32_bops[:-1], fit_computerw.names, w_config)):\n",
    "        if name in fit_computera.names:\n",
    "            bops += bo*a_config[a_indx]*wb*(1/32)**2\n",
    "            a_indx += 1\n",
    "        else:\n",
    "            bops += bo*wb*8*(1/32)**2\n",
    "        \n",
    "#     possible_configs.append((bops, fit_value, w_config, a_config[1:]))\n",
    "    bops_acc.append(bops)\n",
    "    fit_values_acc.append(fit_value)\n",
    "    w_configs_acc.append(w_config)\n",
    "    a_configs_acc.append(a_config)\n",
    "    \n",
    "    if bops < min_criterion and fit_value < min_FIT:\n",
    "        min_FIT = fit_value\n",
    "        best = (bops, fit_value, w_config, a_config)\n",
    "        print('updated')\n",
    "        \n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90acc9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bops_acc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dca158",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.scatter(bops_acc, fit_values_acc, s=2,)\n",
    "plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "# plt.xlim(total_bops * (7.6/32)**2, total_bops * (10/32)**2)\n",
    "# plt.ylim(6,10)\n",
    "plt.scatter(filtered_info[idx][0], filtered_info[idx][1], s=20, marker='v', c='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4206c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(np.arange(len(best[2])), best[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(best[3])), best[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7998335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_config = [8 for i in range(48)]\n",
    "test_fit = fit_computerw.FIT(np.array(w_config), np.array(a_config))\n",
    "w_config = [4 for i in range(72)]\n",
    "test_bops = 0\n",
    "a_config.insert(0, 32)\n",
    "\n",
    "a_indx = 0\n",
    "for i, (bo, name, wb) in enumerate(zip(bert_fp32_bops[:-1], fit_computerw.names, w_config)):\n",
    "    if name in fit_computera.names:\n",
    "        test_bops += bo*a_config[a_indx]*wb*(1/32)**2\n",
    "        a_indx += 1\n",
    "    else:\n",
    "        test_bops += bo*wb*8*(1/32)**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a9ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_bops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6398a9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bops_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m filtered \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m filtered_info \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bps, fit, wconf, aconf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mbops_acc\u001b[49m, fit_values_acc, w_configs_acc, a_configs_acc):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bps \u001b[38;5;241m<\u001b[39m criterion:\n\u001b[1;32m      6\u001b[0m         filtered\u001b[38;5;241m.\u001b[39mappend(fit)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bops_acc' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = 10030593057.0\n",
    "filtered = []\n",
    "filtered_info = []\n",
    "for bps, fit, wconf, aconf in zip(bops_acc, fit_values_acc, w_configs_acc, a_configs_acc):\n",
    "    if bps < criterion:\n",
    "        filtered.append(fit)\n",
    "        filtered_info.append((bps, fit, wconf, aconf))\n",
    "idx = np.argmin(filtered)\n",
    "print(filtered_info[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec833d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_bits = {}\n",
    "for l, b in zip(names[:-1], best[2]):\n",
    "    layer_bits[l[13:]] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layer_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_bits = {}\n",
    "for l, b in zip(fit_computera.names, best[3]):\n",
    "    activation_bits[l[13:]] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047fb5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(activation_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f88d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
