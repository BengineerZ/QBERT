{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84039ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert.modeling import BertConfig\n",
    "\n",
    "# from pytorch_pretrained_bert.modeling import BertForSequenceClassification\n",
    "# from modeling import BertForSequenceClassification_Quant as BertForSequenceClassification\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "\n",
    "FORMAT = '[%(asctime)-15s %(filename)s:%(lineno)s] %(message)s'\n",
    "FORMAT_MINIMAL = '%(message)s'\n",
    "\n",
    "logging.basicConfig(format=FORMAT)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4bb0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_classifier import *\n",
    "from FIT_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from quant_modules import QuantLinear, QuantLinear_Act, QuantEmbedding\n",
    "from transformers import BertForSequenceClassification\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e6eb548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c493bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = type('MyClass', (object,), {'content':{}})()\n",
    "args.data_dir = '/home/ben/Documents/CERN/rebuttal_iclr/GLUE/SST-2/'\n",
    "args.bert_model = 'bert-base-uncased'\n",
    "args.do_lower_case = True\n",
    "args.train_batch_size = 4\n",
    "args.max_seq_length = 128\n",
    "args.task_name = 'SST-2'\n",
    "args.cache_dir = None\n",
    "args.config = None\n",
    "args.config_dir = None\n",
    "args.local_rank=-1\n",
    "args.bit_options = [2,3,4,8,32]\n",
    "args.load_directory = '/home/ben/Documents/CERN/rebuttal_iclr/sst_pretrained/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c9640c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processors = {\n",
    "    \"cola\": ColaProcessor,\n",
    "    \"mnli\": MnliProcessor,\n",
    "    \"mnli-mm\": MnliMismatchedProcessor,\n",
    "    \"mrpc\": MrpcProcessor,\n",
    "    \"sst-2\": Sst2Processor,\n",
    "    \"sts-b\": StsbProcessor,\n",
    "    \"qqp\": QqpProcessor,\n",
    "    \"qnli\": QnliProcessor,\n",
    "    \"rte\": RteProcessor,\n",
    "    \"wnli\": WnliProcessor,\n",
    "}\n",
    "\n",
    "output_modes = {\n",
    "    \"cola\": \"classification\",\n",
    "    \"mnli\": \"classification\",\n",
    "    \"mrpc\": \"classification\",\n",
    "    \"sst-2\": \"classification\",\n",
    "    \"sts-b\": \"regression\",\n",
    "    \"qqp\": \"classification\",\n",
    "    \"qnli\": \"classification\",\n",
    "    \"rte\": \"classification\",\n",
    "    \"wnli\": \"classification\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a0ab163",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = args.task_name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "216c5300",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = processors[task_name]()\n",
    "output_mode = output_modes[task_name]\n",
    "\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ff616f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoModelForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model = BertForSequenceClassification.from_pretrained(\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#             args.load_directory, num_labels=num_labels, config=args.config)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoyoungkim/bert-base-uncased-finetuned-sst2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      5\u001b[0m     args\u001b[38;5;241m.\u001b[39mload_directory, do_lower_case\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdo_lower_case)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForSequenceClassification' is not defined"
     ]
    }
   ],
   "source": [
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#             args.load_directory, num_labels=num_labels, config=args.config)\n",
    "model = BertForSequenceClassification.from_pretrained(args.bert_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e007f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.load_directory, do_lower_case=args.do_lower_case)\n",
    "model = BertForSequenceClassification.from_pretrained(\"doyoungkim/bert-base-uncased-finetuned-sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd9a2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57931275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Prepare model\n",
    "# cache_dir = os.path.join(\n",
    "#     str(PYTORCH_PRETRAINED_BERT_CACHE),\n",
    "#     'distributed_{}'.format(args.local_rank))\n",
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#     args.bert_model,\n",
    "#     cache_dir=cache_dir,\n",
    "#     config_dir=args.config_dir,\n",
    "#     config=args.config,\n",
    "#     num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f5faab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = processor.get_train_examples(args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2678627",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = args.cache_dir if args.cache_dir else os.path.join(\n",
    "        str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(\n",
    "            args.local_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b2d020f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94b81c7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-11-10 20:25:15,700 run_classifier.py:431] Writing example 0 of 67349\n",
      "[2022-11-10 20:25:15,702 run_classifier.py:496] *** Example ***\n",
      "[2022-11-10 20:25:15,702 run_classifier.py:497] guid: train-1\n",
      "[2022-11-10 20:25:15,703 run_classifier.py:498] tokens: [CLS] hide new secret ##ions from the parental units [SEP]\n",
      "[2022-11-10 20:25:15,703 run_classifier.py:499] input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[2022-11-10 20:25:15,703 run_classifier.py:501] input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[2022-11-10 20:25:15,704 run_classifier.py:503] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[2022-11-10 20:25:15,704 run_classifier.py:505] label: 0 (id = 0)\n",
      "[2022-11-10 20:25:15,705 run_classifier.py:496] *** Example ***\n",
      "[2022-11-10 20:25:15,705 run_classifier.py:497] guid: train-2\n",
      "[2022-11-10 20:25:15,705 run_classifier.py:498] tokens: [CLS] contains no wit , only labor ##ed gag ##s [SEP]\n",
      "[2022-11-10 20:25:15,706 run_classifier.py:499] input_ids: 101 3397 2053 15966 1010 2069 4450 2098 18201 2015 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[2022-11-10 20:25:15,706 run_classifier.py:501] input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[2022-11-10 20:25:15,706 run_classifier.py:503] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[2022-11-10 20:25:15,707 run_classifier.py:505] label: 0 (id = 0)\n",
      "[2022-11-10 20:25:15,707 run_classifier.py:496] *** Example ***\n",
      "[2022-11-10 20:25:15,708 run_classifier.py:497] guid: train-3\n",
      "[2022-11-10 20:25:15,708 run_classifier.py:498] tokens: [CLS] that loves its characters and communicate ##s something rather beautiful about human nature [SEP]\n",
      "[2022-11-10 20:25:15,708 run_classifier.py:499] input_ids: 101 2008 7459 2049 3494 1998 10639 2015 2242 2738 3376 2055 2529 3267 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[2022-11-10 20:25:15,709 run_classifier.py:501] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[2022-11-10 20:25:15,709 run_classifier.py:503] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[2022-11-10 20:25:15,709 run_classifier.py:505] label: 1 (id = 1)\n",
      "[2022-11-10 20:25:15,710 run_classifier.py:496] *** Example ***\n",
      "[2022-11-10 20:25:15,710 run_classifier.py:497] guid: train-4\n",
      "[2022-11-10 20:25:15,711 run_classifier.py:498] tokens: [CLS] remains utterly satisfied to remain the same throughout [SEP]\n",
      "[2022-11-10 20:25:15,711 run_classifier.py:499] input_ids: 101 3464 12580 8510 2000 3961 1996 2168 2802 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[2022-11-10 20:25:15,711 run_classifier.py:501] input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[2022-11-10 20:25:15,712 run_classifier.py:503] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[2022-11-10 20:25:15,712 run_classifier.py:505] label: 0 (id = 0)\n",
      "[2022-11-10 20:25:15,712 run_classifier.py:496] *** Example ***\n",
      "[2022-11-10 20:25:15,713 run_classifier.py:497] guid: train-5\n",
      "[2022-11-10 20:25:15,713 run_classifier.py:498] tokens: [CLS] on the worst revenge - of - the - ne ##rds cl ##iche ##s the filmmakers could dr ##edge up [SEP]\n",
      "[2022-11-10 20:25:15,713 run_classifier.py:499] input_ids: 101 2006 1996 5409 7195 1011 1997 1011 1996 1011 11265 17811 18856 17322 2015 1996 16587 2071 2852 24225 2039 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[2022-11-10 20:25:15,714 run_classifier.py:501] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[2022-11-10 20:25:15,714 run_classifier.py:503] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[2022-11-10 20:25:15,714 run_classifier.py:505] label: 0 (id = 0)\n",
      "[2022-11-10 20:25:16,617 run_classifier.py:431] Writing example 10000 of 67349\n",
      "[2022-11-10 20:25:17,540 run_classifier.py:431] Writing example 20000 of 67349\n",
      "[2022-11-10 20:25:18,637 run_classifier.py:431] Writing example 30000 of 67349\n",
      "[2022-11-10 20:25:19,535 run_classifier.py:431] Writing example 40000 of 67349\n",
      "[2022-11-10 20:25:20,426 run_classifier.py:431] Writing example 50000 of 67349\n",
      "[2022-11-10 20:25:21,556 run_classifier.py:431] Writing example 60000 of 67349\n"
     ]
    }
   ],
   "source": [
    "train_features = convert_examples_to_features(\n",
    "    train_examples, label_list, args.max_seq_length, tokenizer,\n",
    "    output_mode)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features],\n",
    "                             dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features],\n",
    "                              dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features],\n",
    "                               dtype=torch.long)\n",
    "\n",
    "if output_mode == \"classification\":\n",
    "    all_label_ids = torch.tensor([f.label_id for f in train_features],\n",
    "                                 dtype=torch.long)\n",
    "elif output_mode == \"regression\":\n",
    "    all_label_ids = torch.tensor([f.label_id for f in train_features],\n",
    "                                 dtype=torch.float)\n",
    "\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask,\n",
    "                           all_segment_ids, all_label_ids)\n",
    "if args.local_rank == -1:\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "else:\n",
    "    train_sampler = DistributedSampler(train_data)\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=args.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43113101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42b80c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define useful layer hooks:\n",
    "def linear_flops_counter_hook(module, input, output):\n",
    "    input = input[0]\n",
    "    # pytorch checks dimensions, so here we don't care much\n",
    "    output_last_dim = output.shape[-1]\n",
    "    bias_flops = output_last_dim if module.bias is not None else 0\n",
    "    module.__flops__ += int(np.prod(input.shape) * output_last_dim + bias_flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75885f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODULES_MAPPING = {\n",
    "    nn.Linear: linear_flops_counter_hook,\n",
    "    QuantLinear: linear_flops_counter_hook,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f61bf758",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "names = []\n",
    "for name, module in model.named_modules():\n",
    "    if type(module) in MODULES_MAPPING:\n",
    "        names.append(name)\n",
    "        layers.append(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19adee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bert.encoder.layer.0.attention.self.query\n",
      "1 bert.encoder.layer.0.attention.self.key\n",
      "2 bert.encoder.layer.0.attention.self.value\n",
      "3 bert.encoder.layer.0.attention.output.dense\n",
      "4 bert.encoder.layer.0.intermediate.dense\n",
      "5 bert.encoder.layer.0.output.dense\n",
      "6 bert.encoder.layer.1.attention.self.query\n",
      "7 bert.encoder.layer.1.attention.self.key\n",
      "8 bert.encoder.layer.1.attention.self.value\n",
      "9 bert.encoder.layer.1.attention.output.dense\n",
      "10 bert.encoder.layer.1.intermediate.dense\n",
      "11 bert.encoder.layer.1.output.dense\n",
      "12 bert.encoder.layer.2.attention.self.query\n",
      "13 bert.encoder.layer.2.attention.self.key\n",
      "14 bert.encoder.layer.2.attention.self.value\n",
      "15 bert.encoder.layer.2.attention.output.dense\n",
      "16 bert.encoder.layer.2.intermediate.dense\n",
      "17 bert.encoder.layer.2.output.dense\n",
      "18 bert.encoder.layer.3.attention.self.query\n",
      "19 bert.encoder.layer.3.attention.self.key\n",
      "20 bert.encoder.layer.3.attention.self.value\n",
      "21 bert.encoder.layer.3.attention.output.dense\n",
      "22 bert.encoder.layer.3.intermediate.dense\n",
      "23 bert.encoder.layer.3.output.dense\n",
      "24 bert.encoder.layer.4.attention.self.query\n",
      "25 bert.encoder.layer.4.attention.self.key\n",
      "26 bert.encoder.layer.4.attention.self.value\n",
      "27 bert.encoder.layer.4.attention.output.dense\n",
      "28 bert.encoder.layer.4.intermediate.dense\n",
      "29 bert.encoder.layer.4.output.dense\n",
      "30 bert.encoder.layer.5.attention.self.query\n",
      "31 bert.encoder.layer.5.attention.self.key\n",
      "32 bert.encoder.layer.5.attention.self.value\n",
      "33 bert.encoder.layer.5.attention.output.dense\n",
      "34 bert.encoder.layer.5.intermediate.dense\n",
      "35 bert.encoder.layer.5.output.dense\n",
      "36 bert.encoder.layer.6.attention.self.query\n",
      "37 bert.encoder.layer.6.attention.self.key\n",
      "38 bert.encoder.layer.6.attention.self.value\n",
      "39 bert.encoder.layer.6.attention.output.dense\n",
      "40 bert.encoder.layer.6.intermediate.dense\n",
      "41 bert.encoder.layer.6.output.dense\n",
      "42 bert.encoder.layer.7.attention.self.query\n",
      "43 bert.encoder.layer.7.attention.self.key\n",
      "44 bert.encoder.layer.7.attention.self.value\n",
      "45 bert.encoder.layer.7.attention.output.dense\n",
      "46 bert.encoder.layer.7.intermediate.dense\n",
      "47 bert.encoder.layer.7.output.dense\n",
      "48 bert.encoder.layer.8.attention.self.query\n",
      "49 bert.encoder.layer.8.attention.self.key\n",
      "50 bert.encoder.layer.8.attention.self.value\n",
      "51 bert.encoder.layer.8.attention.output.dense\n",
      "52 bert.encoder.layer.8.intermediate.dense\n",
      "53 bert.encoder.layer.8.output.dense\n",
      "54 bert.encoder.layer.9.attention.self.query\n",
      "55 bert.encoder.layer.9.attention.self.key\n",
      "56 bert.encoder.layer.9.attention.self.value\n",
      "57 bert.encoder.layer.9.attention.output.dense\n",
      "58 bert.encoder.layer.9.intermediate.dense\n",
      "59 bert.encoder.layer.9.output.dense\n",
      "60 bert.encoder.layer.10.attention.self.query\n",
      "61 bert.encoder.layer.10.attention.self.key\n",
      "62 bert.encoder.layer.10.attention.self.value\n",
      "63 bert.encoder.layer.10.attention.output.dense\n",
      "64 bert.encoder.layer.10.intermediate.dense\n",
      "65 bert.encoder.layer.10.output.dense\n",
      "66 bert.encoder.layer.11.attention.self.query\n",
      "67 bert.encoder.layer.11.attention.self.key\n",
      "68 bert.encoder.layer.11.attention.self.value\n",
      "69 bert.encoder.layer.11.attention.output.dense\n",
      "70 bert.encoder.layer.11.intermediate.dense\n",
      "71 bert.encoder.layer.11.output.dense\n",
      "72 bert.pooler.dense\n",
      "73 classifier\n"
     ]
    }
   ],
   "source": [
    "param_nums = []\n",
    "params = []\n",
    "names = []\n",
    "for name, module in model.named_modules():\n",
    "    if (isinstance(module, nn.Linear) or (isinstance(module, nn.MultiheadAttention))):\n",
    "        for n, p in list(module.named_parameters()):\n",
    "            if n.endswith('weight'):\n",
    "                names.append(name)\n",
    "                p.collect = True\n",
    "                layers.append(module)\n",
    "                param_nums.append(p.numel())\n",
    "                params.append(p)\n",
    "            else:\n",
    "                p.collect = False\n",
    "        continue\n",
    "    for p in list(module.parameters()):\n",
    "        if p.requires_grad:\n",
    "            p.collect = False\n",
    "for i, n in enumerate(names):\n",
    "    print(i, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306a58bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "874e95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarking_Fish(model, device, params, metric, data_loader,iterations):\n",
    "    ''' Used to generate the convergence statistics for the Empirical Fisher\n",
    "    Args:\n",
    "        model\n",
    "        device\n",
    "        params - list of accumulated model parameters to generate placeholders\n",
    "        criterion - loss function used to compute the EF\n",
    "        data_loader\n",
    "        iterations - total number of Estimator iterations to sum over\n",
    "    Returns:\n",
    "        F_average - computed EF trace\n",
    "        estimator_accumulation - accumulation of each individual EF estimator\n",
    "        estimator_mean_accumulation - accumulation of the EF trace estimator over iterations\n",
    "    '''\n",
    "    model.eval()\n",
    "    \n",
    "    # accumulate hutchinson estimator\n",
    "    estimator_accumulation = []\n",
    "\n",
    "    estimator_mean_accumulation = []\n",
    "    \n",
    "    iteration = 0\n",
    "    total_processed = 0\n",
    "    batches = 0\n",
    "    \n",
    "    while(iteration < iterations):\n",
    "\n",
    "        TFv = [torch.zeros(p.size()).to(device) for p in params]  # accumulate iteration up to datapoints_per_iteration\n",
    "        \n",
    "        for i, data in enumerate(data_loader, 1):\n",
    "            batch = tuple(t.to(device) for t in data)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "            # define a new function to compute loss values for both output_modes\n",
    "            logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "            loss = criterion(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "            \n",
    "            loss.backward(create_graph=True)\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            paramsH = []\n",
    "            gradsH = []\n",
    "            for paramH in model.parameters():\n",
    "                if not paramH.collect:\n",
    "                    continue\n",
    "                paramsH.append(paramH)\n",
    "                gradsH.append(0. if paramH.grad is None else paramH.grad + 0.)\n",
    "            \n",
    "            # Fisher Accumulation\n",
    "            G2 = []\n",
    "            for g in gradsH:\n",
    "                G2.append(batch_size*g*g)\n",
    "                \n",
    "            TFv = [TFv_ + G2_ + 0. for TFv_, G2_ in zip(TFv, G2)]\n",
    "            \n",
    "            total_processed += 1\n",
    "            \n",
    "            TFv_normed = [TFv_ / float(total_processed) for TFv_ in TFv]\n",
    "\n",
    "            vFv = [torch.sum(x) for x in TFv_normed]\n",
    "            \n",
    "            indiv = np.array([torch.sum(x).detach().cpu().numpy() for x in G2])\n",
    "            estimator_accumulation.append(indiv)\n",
    "\n",
    "            vFv_c = np.array([i.detach().cpu().numpy() for i in vFv])\n",
    "            \n",
    "            F_average = vFv_c\n",
    "\n",
    "            print(f'Iteration {iteration}')\n",
    "                    \n",
    "            estimator_mean_accumulation.append(F_average)\n",
    "            \n",
    "            iteration += 1\n",
    "            \n",
    "            if iteration >= iterations:\n",
    "                break\n",
    "                \n",
    "#         eval_metric = metric.compute()\n",
    "#         print(f\"Sanity check: {task_name}, {eval_metric}\")\n",
    "\n",
    "    return F_average, estimator_accumulation, estimator_mean_accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ca23fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarking_Hess(model, device, params, metric, data_loader, iterations, datapoints_per_iteration):\n",
    "    ''' Used to generate the convergence statistics for the Hessian\n",
    "    Args:\n",
    "        model\n",
    "        device\n",
    "        params - list of accumulated model parameters to generate placeholders\n",
    "        criterion - loss function used to compute the Hessian\n",
    "        data_loader\n",
    "        iterations - total number of Estimator iterations to sum over\n",
    "        datapoints_per_iteration - min(batch size, datapoints_per_iteration) used to compute each estimate\n",
    "    Returns:\n",
    "        H_average - computed Hessian trace\n",
    "        estimator_accumulation - accumulation of each individual hutchinson estimator\n",
    "        estimator_mean_accumulation - accumulation of the Hutchinson trace estimator over iterations\n",
    "    '''\n",
    "    \n",
    "    ## Defines the Rademacher generation\n",
    "    def rademacher():\n",
    "        v = [torch.randint_like(p, high=2, device=device) for p in params]\n",
    "        for v_i in v:\n",
    "            v_i[v_i == 0] = -1\n",
    "        return v\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # accumulate hutchinson estimator\n",
    "    estimator_accumulation = []\n",
    "    estimator_mean_accumulation = []\n",
    "\n",
    "    iteration = 0\n",
    "    iteration_batch = 0\n",
    "    \n",
    "    v = rademacher()\n",
    "    \n",
    "    while(iteration < iterations):\n",
    "\n",
    "        THv = [torch.zeros(p.size()).to(device) for p in params]\n",
    "        \n",
    "        for i, data in enumerate(data_loader, 1):\n",
    "            batch = tuple(t.to(device) for t in data)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "            # define a new function to compute loss values for both output_modes\n",
    "            logits = model(input_ids, segment_ids, input_mask, labels=None).logits\n",
    "#             loss = logits.loss\n",
    "            loss = criterion(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "            \n",
    "            loss.backward(create_graph=True, retain_graph=True)\n",
    "\n",
    "            paramsH = []\n",
    "            gradsH = []\n",
    "            for paramH in model.parameters():\n",
    "                if not paramH.collect:\n",
    "                    continue\n",
    "                paramsH.append(paramH)\n",
    "                gradsH.append(0. if paramH.grad is None else paramH.grad + 0.)\n",
    "            \n",
    "            Hv = torch.autograd.grad(gradsH, paramsH, grad_outputs=v,only_inputs=False,retain_graph=True)\n",
    "            \n",
    "            THv = [THv_ + Hv_ + 0. for THv_, Hv_ in zip(THv, Hv)]\n",
    "            \n",
    "            iteration_batch += 1\n",
    "            \n",
    "            if iteration_batch*batch_size >= datapoints_per_iteration:\n",
    "                \n",
    "                THv = [THv_ / float(iteration_batch) for THv_ in THv] # normalise to the number of batches\n",
    "                \n",
    "                vHv = [torch.sum(x * y) for (x, y) in zip(THv, v)] # compute the Hutchinson estimator\n",
    "                \n",
    "                vHv_c = np.array([i.cpu().numpy() for i in vHv])\n",
    "                \n",
    "                estimator_accumulation.append(vHv_c) # accumulate the estimator\n",
    "                \n",
    "                H_average = np.mean(estimator_accumulation, axis=0)\n",
    "\n",
    "                estimator_mean_accumulation.append(H_average)\n",
    "                \n",
    "                print(f'Iteration {iteration}')\n",
    "                \n",
    "                # Reset the hutchinson estimator variables\n",
    "                v = rademacher()\n",
    "                THv = [torch.zeros(p.size()).to(device) for p in params]  # accumulate result\n",
    "                iteration_batch = 0\n",
    "                iteration += 1\n",
    "                \n",
    "                if iteration >= iterations:\n",
    "                    break\n",
    "                    \n",
    "    return H_average, estimator_accumulation, estimator_mean_accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da8f9645",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"glue\", 'sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab5aa5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "batch = tuple(t.to(device) for t in batch)\n",
    "input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "# define a new function to compute loss values for both output_modes\n",
    "_ = model(input_ids, segment_ids, input_mask, labels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbfeaa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8fdf5314",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SequenceClassifierOutput' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m F, Fa, Fma \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmarking_Fish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36mbenchmarking_Fish\u001b[0;34m(model, device, params, metric, data_loader, iterations)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# define a new function to compute loss values for both output_modes\u001b[39;00m\n\u001b[1;32m     35\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_ids, segment_ids, input_mask, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 36\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_labels), label_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward(create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SequenceClassifierOutput' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "F, Fa, Fma = benchmarking_Fish(model, device, params, criterion, train_dataloader, iterations = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "956ba2de",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m H, Ha, Hma \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmarking_Hess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdatapoints_per_iteration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36mbenchmarking_Hess\u001b[0;34m(model, device, params, metric, data_loader, iterations, datapoints_per_iteration)\u001b[0m\n\u001b[1;32m     55\u001b[0m     paramsH\u001b[38;5;241m.\u001b[39mappend(paramH)\n\u001b[1;32m     56\u001b[0m     gradsH\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0.\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m paramH\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m paramH\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m Hv \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradsH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamsH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43monly_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m THv \u001b[38;5;241m=\u001b[39m [THv_ \u001b[38;5;241m+\u001b[39m Hv_ \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m THv_, Hv_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(THv, Hv)]\n\u001b[1;32m     62\u001b[0m iteration_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/CERN/lib/python3.9/site-packages/torch/autograd/__init__.py:300\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "H, Ha, Hma = benchmarking_Hess(model, device, params, metric, train_dataloader, \n",
    "                      iterations = 200,  \n",
    "                      datapoints_per_iteration = args.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74436517",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_normed = (Fa - np.mean(Fa, axis=0))/np.mean(Fa, axis=0)\n",
    "# plt.yscale('log')\n",
    "print(f'EF variance: {np.var(mean_normed)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b82d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_normed = (Ha - np.mean(Ha, axis=0))/np.mean(Ha, axis=0)\n",
    "# plt.yscale('log')\n",
    "print(f'Hessian variance: {np.var(mean_normed)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
